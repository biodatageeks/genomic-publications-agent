{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "import faiss\n",
    "import numpy as np\n",
    "import getpass\n",
    "import os\n",
    "\n",
    "from src.PubmedEndpoint import PubmedEndpoint\n",
    "\n",
    "print(\"Step 1: Load the FAISS index and the terms\")\n",
    "index = faiss.read_index(\"ontology_faiss_index.index\")\n",
    "terms = np.load(\"ontology_terms.npy\", allow_pickle=True).tolist()\n",
    "\n",
    "print(\"Step 2: Load the embedding model and prepare FAISS store\")\n",
    "embedding_model = OpenAIEmbeddings()\n",
    "faiss_store = FAISS(embedding_model, index, terms)\n",
    "\n",
    "def chunk_text(text, chunk_size, overlap):\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = start + chunk_size\n",
    "        chunk = text[start:end]\n",
    "        chunks.append(chunk)\n",
    "        start += chunk_size - overlap\n",
    "    return chunks\n",
    "\n",
    "\n",
    "print(\"Step 4: Initialize the LangChain components\")\n",
    "if \"OPENAI_API_KEY\" not in os.environ:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API key: \")\n",
    "memory = ConversationBufferMemory()\n",
    "llm = OpenAI(model=\"gpt-4o-mini\", temperature=0.7)  # Assuming OpenAI model is accessible\n",
    "prompt = ChatPromptTemplate.from_template(\"{question}\\n\\nContext:\\n{context}\")\n",
    "\n",
    "\n",
    "def process_publication(question, publication, chunk_size=1000, overlap=200):\n",
    "    # Split publication into chunks\n",
    "    chunks = chunk_text(publication, chunk_size, overlap)\n",
    "\n",
    "    responses = []\n",
    "    # Iterate through each chunk, making a query to the model\n",
    "    for chunk in chunks:\n",
    "        chain = ConversationalRetrievalChain.from_llm(\n",
    "            llm=llm,\n",
    "            retriever=faiss_store.as_retriever(),\n",
    "            memory=memory,\n",
    "            prompt=prompt\n",
    "        )\n",
    "        response = chain.run(input={\"question\": question, \"context\": chunk})\n",
    "        responses.append(response)\n",
    "    return responses\n",
    "\n",
    "\n",
    "print(\"Step 5: Process a publication\")\n",
    "\n",
    "pubmed_id = '24212882'\n",
    "text = PubmedEndpoint.fetch_text_from_pubmed_id(pubmed_id)\n",
    "question = (\"Please give me all fragments of text (of length circa 25 words) where there are genomic region \"\n",
    "            \"coordinates in the format like here or similar: chr10:23508365, chrY:∼124349-409949; chrY:∼134349–439949\")\n",
    "responses = process_publication(question, text)\n",
    "with open(\"out/responses.txt\", \"w+\") as file:\n",
    "    for item in responses:\n",
    "        file.write(\"%s\\n\" % item)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
